{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf64a7b0-010a-4541-a789-f968c7865c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Junyu Zhu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5273ed0-0458-454e-831a-1d992e798435",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I use generated fake data instead to show/test the project since notebook lacks some module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dd2331-71b9-4f4f-99a5-83df5717aa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "#from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import entropy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sqlite3\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6291b1-56c5-4e8d-85d2-f9a5f223d0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1: Getting data(Not used here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85c6ce1-d1a1-4501-b099-812f8c48a6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read posts data\n",
    "posts_df = pd.read_csv('sample_posts.txt', sep='\\t')\n",
    "print(\"Posts Data:\")\n",
    "print(posts_df.head())\n",
    "\n",
    "# Read survey data\n",
    "survey_df = pd.read_csv('sample_survey.txt', sep='\\t')\n",
    "print(\"\\nSurvey Data:\")\n",
    "print(survey_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9319ecb-1918-426d-8736-cb12e24bd1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1: Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4bf4d8-ecd1-45aa-a7f2-0e611c15f2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Fake Data\n",
    "def generate_fake_data(platform, num_samples):\n",
    "    fake_data = []\n",
    "    for _ in range(num_samples):\n",
    "        fake_data.append({\n",
    "            \"platform\": platform,\n",
    "            \"text\": f\"This is a sample post about {random.choice(['topic one', 'topic two', 'topic three', 'topic four', 'topic five'])}.\",\n",
    "            \"created_at\": pd.Timestamp.now() - pd.to_timedelta(random.randint(1, 1000), unit='h'),\n",
    "            \"user_id\": random.randint(1000, 9999)\n",
    "        })\n",
    "    return pd.DataFrame(fake_data)\n",
    "\n",
    "# Combine fake data from multiple platforms\n",
    "def create_fake_dataset():\n",
    "    platforms = [\"Twitter\", \"Facebook\", \"Instagram\", \"Bluesky\", \"Mastodon\"]\n",
    "    data_frames = [generate_fake_data(platform, 100) for platform in platforms]\n",
    "    return pd.concat(data_frames, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd03bf13-f3be-40b2-abcd-f94a11f8ebe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the dataset and print the first few rows\n",
    "data = create_fake_dataset()\n",
    "print(\"Generated Dataset (First 5 Rows):\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7f1cc9-8498-40da-bb1e-ffb01fc84ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Fake Survey Data\n",
    "def generate_survey_data(num_responses=100):\n",
    "    survey_data = []\n",
    "    for _ in range(num_responses):\n",
    "        response = {\n",
    "            \"user_id\": random.randint(1000, 9999),\n",
    "            \"platform\": random.choice([\"Twitter\", \"Facebook\", \"Instagram\", \"Bluesky\", \"Mastodon\"]),\n",
    "            \"civility_score\": random.uniform(1, 5),\n",
    "            \"usability_score\": random.uniform(1, 5),\n",
    "            \"echo_chamber_perception\": random.choice([True, False]),\n",
    "            \"intelligence\": random.uniform(80, 160)  # IQ-like metric\n",
    "        }\n",
    "        survey_data.append(response)\n",
    "    return pd.DataFrame(survey_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be984a5-c2ac-45a7-abd1-b0830df0e27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the dataset and print the first few rows\n",
    "survey_data = generate_survey_data()\n",
    "print(\"Generated Dataset (First 5 Rows):\")\n",
    "print(survey_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac321db-75ad-4506-8114-e6589fc21204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2: Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d808fd8-159f-4d1c-8097-26f98cc3f358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and preprocess text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n",
    "    text = re.sub(r\"@\\w+|\\#\\w+\", \"\", text)\n",
    "    text = re.sub(r\"[^A-Za-z\\s]\", \"\", text)\n",
    "    text = text.lower()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc511c2-9539-4b6e-833f-c7d1bafcc00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = word_tokenize(text)\n",
    "    return \" \".join([word for word in tokens if word not in stop_words])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0aedcf-fd92-44b5-8a10-372b0a5cc2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and preprocess the data\n",
    "data[\"clean_text\"] = data[\"text\"].apply(clean_text).apply(preprocess_text)\n",
    "print(\"\\nCleaned and Preprocessed Text (First 5 Rows):\")\n",
    "print(data[[\"text\", \"clean_text\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40d226c-00ee-4aa7-a785-08adfce61db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3: Sentiment Analysis(Show only Don't run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b285bde-9690-4d61-8ad7-8d3a66ca9623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize VADER Sentiment Analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define a function to compute civility and intelligence scores\n",
    "def evaluate_via_vader(text):\n",
    "    \"\"\"\n",
    "    Evaluate civility and intelligence using VADER sentiment scores.\n",
    "    Civility Score: Based on positive and negative sentiment.\n",
    "    Intelligence Score: Based on the absolute compound score.\n",
    "    \"\"\"\n",
    "    sentiment = analyzer.polarity_scores(text)\n",
    "    civility_score = max(0, min(5, sentiment[\"pos\"] * 5 - sentiment[\"neg\"] * 5))  # Scale civility between 0-5\n",
    "    intelligence_score = max(0, min(5, abs(sentiment[\"compound\"]) * 5))  # Scale intelligence between 0-5\n",
    "    return civility_score, intelligence_score\n",
    "\n",
    "# Apply the function to the clean_text column\n",
    "data[\"civility_score\"], data[\"intelligence_score\"] = zip(\n",
    "    *data[\"clean_text\"].apply(evaluate_via_vader)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75d57b5-5ccd-46d4-ac27-2da2fa44b5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a preview of the updated DataFrame\n",
    "print(\"\\nUpdated Data with Civility and Intelligence Scores (First 5 Rows):\")\n",
    "print(data[[\"clean_text\", \"civility_score\", \"intelligence_score\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c1df50-27d4-4762-bb92-3add46a7426e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3: Sentiment Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462a59b5-c497-48ef-942e-7a32c3111e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions to generate fake civility and intelligence scores\n",
    "def generate_fake_civility():\n",
    "    \"\"\"Generate a fake civility score between 1 and 5.\"\"\"\n",
    "    return round(random.uniform(1, 5), 2)\n",
    "\n",
    "def generate_fake_intelligence():\n",
    "    \"\"\"Generate a fake intelligence score between 1 and 5.\"\"\"\n",
    "    return round(random.uniform(1, 5), 2)\n",
    "\n",
    "# Apply the functions to generate scores\n",
    "data[\"civility_score\"] = data[\"clean_text\"].apply(lambda x: generate_fake_civility())\n",
    "data[\"intelligence_score\"] = data[\"clean_text\"].apply(lambda x: generate_fake_intelligence())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984c9822-e68b-4a24-8215-e912f4ba67f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a preview of the updated DataFrame\n",
    "print(\"\\nUpdated Data with Fake Civility and Intelligence Scores (First 5 Rows):\")\n",
    "print(data[[\"clean_text\", \"civility_score\", \"intelligence_score\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d4816f-05c8-400c-9b08-200b8bf945c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4: Clustering and Echo Chamber Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1706a144-0ff8-448a-b638-e479b3e807cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect Echo Chambers/Clustering\n",
    "def detect_echo_chambers(texts, num_clusters=3):\n",
    "    #Detect echo chambers by clustering similar content.\n",
    "    #texts: List of text data to cluster.\n",
    "    #num_clusters: Number of clusters to form.\n",
    "    #Returns labels: Cluster labels for each text entry.\n",
    "    \n",
    "    # Step 1: Vectorize text using TF-IDF\n",
    "    vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    # Step 2: Apply K-Means clustering\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f5e96a-4628-4fde-b261-63dab49697f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply clustering for echo chamber detection\n",
    "data[\"cluster\"] = detect_echo_chambers(data[\"clean_text\"])\n",
    "print(\"\\nClustering Results (First 5 Rows):\")\n",
    "print(data[[\"clean_text\", \"cluster\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e539c8-5ce3-4204-bf1f-0cde92f9c6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt: Visualize clusters (Dimensionality Reduction with PCA)\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def plot_clusters(data, texts, cluster_column):\n",
    "    \"\"\"\n",
    "    Visualize the clustering results.\n",
    "\n",
    "    Parameters:\n",
    "    - data: DataFrame containing the data and cluster assignments.\n",
    "    - texts: Original text data (used for dimensionality reduction).\n",
    "    - cluster_column: Column name indicating cluster labels.\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "\n",
    "    # Reduce dimensions for visualization\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced_data = pca.fit_transform(X.toarray())\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(\n",
    "        reduced_data[:, 0], \n",
    "        reduced_data[:, 1], \n",
    "        c=data[cluster_column], \n",
    "        cmap='viridis', \n",
    "        s=10\n",
    "    )\n",
    "    plt.colorbar(label=\"Cluster\")\n",
    "    plt.title(\"Echo Chamber Clusters\")\n",
    "    plt.xlabel(\"PCA Component 1\")\n",
    "    plt.ylabel(\"PCA Component 2\")\n",
    "    plt.show()\n",
    "\n",
    "# Plot the clusters\n",
    "plot_clusters(data, data[\"clean_text\"], \"cluster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857bda95-edf0-46b1-bd2a-d21191e1aaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 5: Algorithm Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a68c6eb-ef86-4a81-a765-777c560f1b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate content recommendation algorithms\n",
    "# Detect algorithm toxicity (frequency of controversial content recommendations)\n",
    "def calculate_algorithm_toxicity(data):\n",
    "    toxicity = data.groupby(\"platform\")[\"civility_score\"].apply(\n",
    "        lambda scores: sum(score < 2.5 for score in scores) / len(scores)\n",
    "    )\n",
    "    toxicity_table = pd.DataFrame({\"platform\": toxicity.index, \"algorithm_toxicity\": toxicity.values})\n",
    "    return toxicity_table\n",
    "\n",
    "# Create algorithm toxicity table\n",
    "toxicity_table = calculate_algorithm_toxicity(data)\n",
    "print(\"\\nAlgorithm Toxicity Table:\")\n",
    "print(toxicity_table)\n",
    "\n",
    "# Graph algorithm toxicity by platform\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=\"platform\", y=\"algorithm_toxicity\", data=toxicity_table, palette=\"coolwarm\")\n",
    "plt.title(\"Algorithm Toxicity by Platform (Controversial Content Frequency)\")\n",
    "plt.xlabel(\"Platform\")\n",
    "plt.ylabel(\"Algorithm Toxicity (Frequency)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af45ba4-b919-48c5-bd31-930f7d579b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 6: Database Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1204d13-de05-4d2e-8b0b-9874a5f23096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database setup\n",
    "def setup_database():\n",
    "    conn = sqlite3.connect(\"social_media_analysis.db\")\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS posts (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            platform TEXT,\n",
    "            text TEXT,\n",
    "            clean_text TEXT,\n",
    "            created_at TEXT,\n",
    "            user_id TEXT,\n",
    "            civility_score REAL,\n",
    "            intelligence_score REAL,\n",
    "            cluster INTEGER\n",
    "        )\n",
    "    \"\"\")\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS survey (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            user_id TEXT,\n",
    "            platform TEXT,\n",
    "            civility_score REAL,\n",
    "            usability_score REAL,\n",
    "            echo_chamber_perception BOOLEAN,\n",
    "            intelligence REAL\n",
    "        )\n",
    "    \"\"\")\n",
    "    conn.commit()\n",
    "    return conn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce780fb-89b7-43e6-acc2-99a9576d5f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert data into the database\n",
    "def store_data(conn, dataframe, table_name):\n",
    "    dataframe.to_sql(table_name, conn, if_exists=\"append\", index=False)\n",
    "\n",
    "# Setup database and insert generated data\n",
    "conn = setup_database()\n",
    "store_data(conn, data, \"posts\")\n",
    "store_data(conn, survey_data, \"survey\")\n",
    "\n",
    "# Verify the data insertion by querying the tables\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT * FROM posts LIMIT 5\")\n",
    "print(\"Posts Table (First 5 Rows):\")\n",
    "for row in cursor.fetchall():\n",
    "    print(row)\n",
    "\n",
    "cursor.execute(\"SELECT * FROM survey LIMIT 5\")\n",
    "print(\"Survey Table (First 5 Rows):\")\n",
    "for row in cursor.fetchall():\n",
    "    print(row)\n",
    "\n",
    "# Close the connection\n",
    "#conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f745405-16b7-4405-bf85-69e3f71d64bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 7: Queries/Database(sql) manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dfae5f-e9b0-4580-93eb-59ff3dab8b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query execution\n",
    "def run_query(query):\n",
    "    conn = sqlite3.connect(\"social_media_analysis.db\")\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(query)\n",
    "    results = cursor.fetchall()\n",
    "    conn.close()\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b12f17f-856f-4b56-b4d1-d4af8d055bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Analyzing relationships with SQL queries...\")\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    p.platform AS platform,\n",
    "    p.cluster AS cluster,\n",
    "    COUNT(p.id) AS post_count\n",
    "FROM posts p\n",
    "GROUP BY p.platform, p.cluster\n",
    "ORDER BY platform, cluster;\n",
    "\"\"\"\n",
    "result = pd.read_sql_query(query, conn)\n",
    "print(\"\\nCluster Distribution Across Platforms:\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f322f29-7937-49b2-9618-5b05735eaed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    p.platform AS platform,\n",
    "    AVG(p.civility_score) AS avg_civility,\n",
    "    AVG(p.intelligence_score) AS avg_intelligence\n",
    "FROM posts p\n",
    "GROUP BY p.platform\n",
    "ORDER BY avg_civility DESC;\n",
    "\"\"\"\n",
    "result = pd.read_sql_query(query, conn)\n",
    "print(\"\\nRelationships Between Civility and Intelligence Levels by Platform:\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e848be7-86c4-495c-b518-942c0969fb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    p.cluster AS cluster,\n",
    "    AVG(p.civility_score) AS avg_civility,\n",
    "    AVG(p.intelligence_score) AS avg_intelligence,\n",
    "    COUNT(p.id) AS post_count\n",
    "FROM posts p\n",
    "GROUP BY p.cluster\n",
    "ORDER BY avg_civility DESC, avg_intelligence DESC;\n",
    "\"\"\"\n",
    "result = pd.read_sql_query(query, conn)\n",
    "print(\"\\nCivility and Intelligence Levels by Clusters:\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b592a385-d784-4f8d-901c-6be0bd14bad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    p.id AS post_id,\n",
    "    p.platform AS post_platform,\n",
    "    p.text AS post_text,\n",
    "    s.platform AS survey_platform,\n",
    "    s.civility_score AS survey_civility,\n",
    "    s.usability_score\n",
    "FROM posts p\n",
    "JOIN survey s ON p.user_id = s.user_id\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "result = pd.read_sql_query(query, conn)\n",
    "print(\"\\nPosts with Survey Data (First 10 Rows):\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f379b1ac-1899-45c6-85c4-bf169a53d686",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    p.platform AS post_platform,\n",
    "    AVG(p.civility_score) AS avg_post_civility,\n",
    "    AVG(s.civility_score) AS avg_survey_civility\n",
    "FROM posts p\n",
    "JOIN survey s ON p.user_id = s.user_id\n",
    "GROUP BY p.platform;\n",
    "\"\"\"\n",
    "result = pd.read_sql_query(query, conn)\n",
    "print(\"\\nAverage Civility by Platform:\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcd01c3-58c4-43a2-993c-710f5137db9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    AVG(s.usability_score) AS avg_usability,\n",
    "    AVG(p.civility_score) AS avg_post_civility\n",
    "FROM survey s\n",
    "JOIN posts p ON s.user_id = p.user_id\n",
    "GROUP BY s.platform;\n",
    "\"\"\"\n",
    "result = pd.read_sql_query(query, conn)\n",
    "print(\"\\nUsability vs. Civility by Platform:\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b746e79-f698-4bef-81bf-ac3aa0b801bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    p.id AS post_id,\n",
    "    p.civility_score AS post_civility,\n",
    "    s.civility_score AS survey_civility,\n",
    "    p.clean_text\n",
    "FROM posts p\n",
    "JOIN survey s ON p.user_id = s.user_id\n",
    "WHERE p.civility_score < 2\n",
    "ORDER BY p.civility_score ASC\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "result = pd.read_sql_query(query, conn)\n",
    "print(\"\\nComparison of Controversial Posts and Survey Civility (First 10 Rows):\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b73e43c-310e-4828-b20c-3b20f160f21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# close database\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6464989-00a1-403e-ab65-dd3fb3176396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 8: Visualization and Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4992197b-e957-48e9-a5d6-52d7f2c94c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Civility Distribution\n",
    "sns.histplot(data[\"civility_score\"], bins=5, kde=False)\n",
    "plt.title(\"Distribution of Civility Scores\")\n",
    "plt.xlabel(\"Civility Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Intelligence vs Civility\n",
    "sns.scatterplot(x=\"civility_score\", y=\"intelligence_score\", hue=\"cluster\", data=data)\n",
    "plt.title(\"Civility vs Intelligence by Cluster\")\n",
    "plt.xlabel(\"Civility Score\")\n",
    "plt.ylabel(\"Intelligence Score\")\n",
    "plt.legend(title=\"Cluster\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096ba57d-6686-4442-88c0-2229aa49defd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster Distribution Across Platforms\n",
    "cluster_distribution = data.groupby([\"platform\", \"cluster\"]).size().reset_index(name=\"count\")\n",
    "total_counts = cluster_distribution.groupby(\"platform\")[\"count\"].transform(\"sum\")\n",
    "cluster_distribution[\"proportion\"] = cluster_distribution[\"count\"] / total_counts\n",
    "\n",
    "print(\"\\nCluster Distribution Across Platforms (Proportion):\")\n",
    "print(cluster_distribution)\n",
    "\n",
    "# Visualization: Stacked Bar Chart for Cluster Distribution\n",
    "platforms = cluster_distribution[\"platform\"].unique()\n",
    "clusters = cluster_distribution[\"cluster\"].unique()\n",
    "cluster_pivot = cluster_distribution.pivot(index=\"platform\", columns=\"cluster\", values=\"proportion\").fillna(0)\n",
    "\n",
    "cluster_pivot.plot(kind=\"bar\", stacked=True, figsize=(10, 6), colormap=\"viridis\")\n",
    "plt.title(\"Cluster Distribution Across Platforms (Proportion)\")\n",
    "plt.xlabel(\"Platform\")\n",
    "plt.ylabel(\"Proportion of Clusters\")\n",
    "plt.legend(title=\"Cluster\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675afab2-2e52-4fdb-9f8f-7d9dc1cb7a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intelligence Scores Across Platforms\n",
    "intelligence = data.groupby(\"platform\")[\"intelligence_score\"].mean().reset_index()\n",
    "print(\"\\nIntelligence Scores Across Platforms:\")\n",
    "print(intelligence)\n",
    "\n",
    "# Visualization: Intelligence Scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=\"platform\", y=\"intelligence_score\", data=intelligence, palette=\"coolwarm\")\n",
    "plt.title(\"Intelligence Scores Across Platforms\")\n",
    "plt.xlabel(\"Platform\")\n",
    "plt.ylabel(\"Average Intelligence Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8381a570-a796-4a5b-8ba1-d944daf2fb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Perception vs Reality (Civility and Intelligence)\n",
    "# Aggregating actual scores from the posts\n",
    "actual_scores = data.groupby(\"platform\").agg({\n",
    "    \"civility_score\": \"mean\",\n",
    "    \"intelligence_score\": \"mean\"\n",
    "}).reset_index()\n",
    "actual_scores.columns = [\"platform\", \"actual_civility_score\", \"actual_intelligence_score\"]\n",
    "\n",
    "# Aggregating survey scores\n",
    "survey_scores = survey_data.groupby(\"platform\").agg({\n",
    "    \"civility_score\": \"mean\",\n",
    "    \"intelligence\": \"mean\"\n",
    "}).reset_index()\n",
    "survey_scores.columns = [\"platform\", \"perceived_civility_score\", \"perceived_intelligence_score\"]\n",
    "\n",
    "# Merge the two datasets for comparison\n",
    "perception_vs_reality = pd.merge(actual_scores, survey_scores, on=\"platform\")\n",
    "\n",
    "print(\"\\nPerception vs Reality of Civility and Intelligence Scores:\")\n",
    "print(perception_vs_reality)\n",
    "\n",
    "# Visualization: Grouped Bar Chart for Perception vs Reality\n",
    "fig, ax = plt.subplots(2, 1, figsize=(12, 10), sharex=True)\n",
    "\n",
    "x = np.arange(len(perception_vs_reality[\"platform\"]))\n",
    "width = 0.35  # Bar width\n",
    "\n",
    "# Civility Comparison\n",
    "ax[0].bar(x - width/2, perception_vs_reality[\"actual_civility_score\"], width, label=\"Actual Civility Score\", color=\"skyblue\")\n",
    "ax[0].bar(x + width/2, perception_vs_reality[\"perceived_civility_score\"], width, label=\"Perceived Civility Score\", color=\"orange\")\n",
    "ax[0].set_title(\"Perception vs Reality of Civility Scores Across Platforms\")\n",
    "ax[0].set_ylabel(\"Civility Score\")\n",
    "ax[0].set_xticks(x)\n",
    "ax[0].set_xticklabels(perception_vs_reality[\"platform\"])\n",
    "ax[0].legend()\n",
    "\n",
    "# Intelligence Comparison\n",
    "ax[1].bar(x - width/2, perception_vs_reality[\"actual_intelligence_score\"], width, label=\"Actual Intelligence Score\", color=\"lightgreen\")\n",
    "ax[1].bar(x + width/2, perception_vs_reality[\"perceived_intelligence_score\"], width, label=\"Perceived Intelligence Score\", color=\"purple\")\n",
    "ax[1].set_title(\"Perception vs Reality of Intelligence Scores Across Platforms\")\n",
    "ax[1].set_ylabel(\"Intelligence Score\")\n",
    "ax[1].set_xticks(x)\n",
    "ax[1].set_xticklabels(perception_vs_reality[\"platform\"])\n",
    "ax[1].legend()\n",
    "\n",
    "plt.xlabel(\"Platform\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fb3a4d-b55f-4711-980b-33c6c0d28de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall User Environment Score Calculation\n",
    "# Calculate overall user environment score with adjusted weights\n",
    "def calculate_overall_environment_weighted(data, survey_data):\n",
    "    # Aggregate civility, intelligence, and cluster information\n",
    "    post_metrics = data.groupby(\"platform\").agg({\n",
    "        \"civility_score\": \"mean\",\n",
    "        \"intelligence_score\": \"mean\",\n",
    "        \"cluster\": \"nunique\"\n",
    "    }).reset_index()\n",
    "    post_metrics.columns = [\"platform\", \"avg_civility_score\", \"avg_intelligence_score\", \"num_clusters\"]\n",
    "\n",
    "    # Add usability from survey data\n",
    "    survey_metrics = survey_data.groupby(\"platform\")[\"usability_score\"].mean().reset_index()\n",
    "    survey_metrics.columns = [\"platform\", \"avg_usability_score\"]\n",
    "\n",
    "    # Combine metrics\n",
    "    overall_metrics = pd.merge(post_metrics, survey_metrics, on=\"platform\")\n",
    "    \n",
    "    # Assign weights: civility and intelligence (0.4 each), usability and clusters (0.1 each), algorithm toxicity (0.2)\n",
    "    civility_weight = 0.4\n",
    "    intelligence_weight = 0.4\n",
    "    usability_weight = 0.1\n",
    "    cluster_weight = 0.1\n",
    "    algorithm_weight = 0.2\n",
    "\n",
    "    overall_metrics[\"overall_score\"] = (\n",
    "        (overall_metrics[\"avg_civility_score\"] * civility_weight) +\n",
    "        (overall_metrics[\"avg_intelligence_score\"] * intelligence_weight) +\n",
    "        (overall_metrics[\"avg_usability_score\"] * usability_weight) -\n",
    "        (toxicity_table[\"algorithm_toxicity\"] * algorithm_weight) +\n",
    "        ((5 - overall_metrics[\"num_clusters\"]) * cluster_weight)  # Inverse cluster count for echo chamber score\n",
    "    )\n",
    "    return overall_metrics\n",
    "\n",
    "# Calculate and display results\n",
    "overall_metrics_weighted = calculate_overall_environment_weighted(data, survey_data)\n",
    "\n",
    "# Graph overall user environment with emphasized app name and score\n",
    "def plot_weighted_overall_environment(overall_metrics):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.axis('off')  # Turn off axes for emphasis on the labels\n",
    "    \n",
    "    # Create a custom text-based display\n",
    "    for i, row in enumerate(overall_metrics.itertuples()):\n",
    "        app_text = f\"**{row.platform}**\"\n",
    "        score_text = f\"{row.overall_score:.2f}\"\n",
    "        plt.text(0.5, 1 - i * 0.15, app_text, fontsize=14, ha=\"center\", color=\"red\", weight=\"bold\")\n",
    "        plt.text(0.5, 1 - i * 0.15 - 0.03, score_text, fontsize=12, ha=\"center\", color=\"black\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "# Call the function to plot\n",
    "plot_weighted_overall_environment(overall_metrics_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6b8b35-64a5-40fd-8e67-acb95d2b8a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 9: Machine Learning: Relationships and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb266e7b-79a2-4219-8c1c-eced32afa16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Civility and Intelligence\n",
    "def analyze_relationships(data):\n",
    "    # Use Linear Regression to analyze relationships.\n",
    "    # Civility and Intelligence Relationship\n",
    "    X = data[[\"civility_score\"]]\n",
    "    y = data[\"intelligence_score\"]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    predictions = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    \n",
    "    print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "    print(f\"R-squared: {r2:.2f}\")\n",
    "    return model\n",
    "\n",
    "relationship_model = analyze_relationships(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d771fae6-e211-4298-819e-73945ee45111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithms and Echo chambers\n",
    "# Use linear regression to analyze the relationship between algorithm toxicity and echo chambers\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def analyze_toxicity_vs_clusters(toxicity_table, data):\n",
    "    cluster_data = data.groupby(\"platform\")[\"cluster\"].nunique().reset_index()\n",
    "    cluster_data.columns = [\"platform\", \"num_clusters\"]\n",
    "    combined_data = pd.merge(toxicity_table, cluster_data, on=\"platform\")\n",
    "\n",
    "    # Regression\n",
    "    X = combined_data[\"algorithm_toxicity\"].values.reshape(-1, 1)\n",
    "    y = combined_data[\"num_clusters\"].values\n",
    "    model = LinearRegression().fit(X, y)\n",
    "\n",
    "    # Graph the relationship\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x=\"algorithm_toxicity\", y=\"num_clusters\", data=combined_data, s=100, color=\"orange\")\n",
    "    plt.plot(combined_data[\"algorithm_toxicity\"], model.predict(X), color=\"blue\", label=\"Regression Line\")\n",
    "    plt.title(\"Relationship Between Algorithm Toxicity and Echo Chambers\")\n",
    "    plt.xlabel(\"Algorithm Toxicity\")\n",
    "    plt.ylabel(\"Number of Clusters (Echo Chambers)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return combined_data, model.coef_, model.intercept_\n",
    "\n",
    "toxicity_vs_clusters_data, coef, intercept = analyze_toxicity_vs_clusters(toxicity_table, data)\n",
    "print(\"\\nCombined Data (Algorithm Toxicity vs. Echo Chambers):\")\n",
    "print(toxicity_vs_clusters_data)\n",
    "print(f\"\\nLinear Regression Coefficient: {coef[0]:.2f}, Intercept: {intercept:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bab145-46dc-43dd-a5a8-f93ec382c261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"\\nFinal Data Sample:\")\n",
    "print(data.head())\n",
    "print(survey_data.head())\n",
    "print(\"\\nOverall User Environment Scores:\")\n",
    "print(overall_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18a0342-fd7c-4409-aab2-129e06d1ab4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
